import chardet
import re
from collections import Counter

def detect_encoding(filepath: str) -> str:
    """–û–ø—Ä–µ–¥–µ–ª—è–µ—Ç –∫–æ–¥–∏—Ä–æ–≤–∫—É —Ñ–∞–π–ª–∞"""
    with open(filepath, "rb") as f:
        raw_data = f.read(100000)  # —á–∏—Ç–∞–µ–º –∫—É—Å–æ–∫ —Ñ–∞–π–ª–∞
    result = chardet.detect(raw_data)
    return result["encoding"] or "utf-8"

def load_text(filepath: str) -> str:
    """–ó–∞–≥—Ä—É–∂–∞–µ—Ç —Ç–µ–∫—Å—Ç —Å –ø—Ä–∞–≤–∏–ª—å–Ω–æ–π –∫–æ–¥–∏—Ä–æ–≤–∫–æ–π"""
    encoding = detect_encoding(filepath)
    try:
        with open(filepath, "r", encoding=encoding) as f:
            return f.read()
    except Exception as e:
        raise RuntimeError(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —á—Ç–µ–Ω–∏–∏ —Ñ–∞–π–ª–∞: {e}")

def analyze_text(text: str) -> dict:
    """–í—ã–ø–æ–ª–Ω—è–µ—Ç –∞–Ω–∞–ª–∏–∑ —Ç–µ–∫—Å—Ç–∞"""
    # –£–±–∏—Ä–∞–µ–º –¥–≤–æ–π–Ω—ã–µ –ø—Ä–æ–±–µ–ª—ã –∏ –ª–∏—à–Ω–∏–µ —Å–∏–º–≤–æ–ª—ã
    cleaned_text = text.strip()

    # –ü–æ–¥—Å—á—ë—Ç —Å–∏–º–≤–æ–ª–æ–≤
    total_chars = len(cleaned_text)
    total_chars_no_spaces = len(cleaned_text.replace(" ", ""))

    # –ü–æ–¥—Å—á—ë—Ç —Å–ª–æ–≤
    words = re.findall(r"\b\w+\b", cleaned_text.lower())
    total_words = len(words)

    # –£–Ω–∏–∫–∞–ª—å–Ω—ã–µ —Å–ª–æ–≤–∞
    unique_words = len(set(words))

    # –ü–æ–¥—Å—á—ë—Ç –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π
    sentences = re.split(r"[.!?]+", cleaned_text)
    sentences = [s.strip() for s in sentences if s.strip()]
    total_sentences = len(sentences)

    # –¢–æ–ø-10 —Å–ª–æ–≤
    counter = Counter(words)
    top_words = counter.most_common(10)

    return {
        "total_words": total_words,
        "total_chars": total_chars,
        "total_chars_no_spaces": total_chars_no_spaces,
        "total_sentences": total_sentences,
        "unique_words": unique_words,
        "top_words": top_words,
    }

def save_report(report_path: str, stats: dict):
    """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç –æ—Ç—á—ë—Ç –≤ —Ñ–∞–π–ª"""
    with open(report_path, "w", encoding="utf-8") as f:
        f.write("üìä –û—Ç—á—ë—Ç –ø–æ –∞–Ω–∞–ª–∏–∑—É —Ç–µ–∫—Å—Ç–∞\n")
        f.write("="*40 + "\n")
        f.write(f"–í—Å–µ–≥–æ —Å–ª–æ–≤: {stats['total_words']}\n")
        f.write(f"–í—Å–µ–≥–æ —Å–∏–º–≤–æ–ª–æ–≤ (—Å –ø—Ä–æ–±–µ–ª–∞–º–∏): {stats['total_chars']}\n")
        f.write(f"–í—Å–µ–≥–æ —Å–∏–º–≤–æ–ª–æ–≤ (–±–µ–∑ –ø—Ä–æ–±–µ–ª–æ–≤): {stats['total_chars_no_spaces']}\n")
        f.write(f"–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π: {stats['total_sentences']}\n")
        f.write(f"–£–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Å–ª–æ–≤: {stats['unique_words']}\n\n")
        f.write("–¢–æ–ø-10 —Å–ª–æ–≤:\n")
        for word, count in stats["top_words"]:
            f.write(f" - {word}: {count}\n")
    print(f"‚úÖ –û—Ç—á—ë—Ç —Å–æ—Ö—Ä–∞–Ω—ë–Ω –≤ {report_path}")

def main():
    filepath = input("–í–≤–µ–¥–∏—Ç–µ –ø—É—Ç—å –∫ —Ç–µ–∫—Å—Ç–æ–≤–æ–º—É —Ñ–∞–π–ª—É: ").strip()
    try:
        text = load_text(filepath)
        stats = analyze_text(text)
        save_report("report.txt", stats)
    except Exception as e:
        print("–û—à–∏–±–∫–∞:", e)

if __name__ == "__main__":
    main()
